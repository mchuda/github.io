---
title: "CMSC320 Final Project"
author: "Maya, Sahana, Akhil"
output: 
  html_document:
    number_sections: true
    toc: true
    


---

# Background and Setting Up

## Background
Graduate School is a goal for many students in University. However, unlike in high school, the resources and information are not as widespread, so prospective young adults have to independently do research to be informed. There are many factors in the application process, and the purpose of this project is to help gain a better understanding on what is needed for a successful higher education.

Data Science provides a manner with which we are able to best figure out what aspects of the application process are the most important. While merely looking at a table can give us a general idea, various tests and visualization can help us conclude what makes an ideal applicant. With this information, we, along with other undergraduate students, can be best prepared.

## Setting up
Prior to accessing our dataset, we need to first load the packages necessary for this study.
```{r setupdb, include=TRUE, comment=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(broom)
library(caret)
library(plotROC)
library(magrittr)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(ggmap)
library(tmap)
library(tmaptools)
library(leaflet)
library(stringr)
library(pracma)
library(rvest)
```


# Parsing

## Scraping

The dataset that we use for this project is on kaggle. However, we wanted to show our ability to scrape data and thus have included an additional data set in our project. Therefore, we have included some of the best Graduate College Universities as an additional datase in order for our readers to have a better understanding of the top schools in the country for Graduate School Programs. This information comes to us from https://www.graduateprograms.com/masters-degrees/best-computer-science-schools


```{r scrape, message=FALSE, warning=FALSE}


url <- "https://thebestschools.org/rankings/20-top-graduate-programs-computer-science-application-process/"


dl_tab <- url %>% 
  read_html() %>%
  html_text() %>%
  str_split("\n")

  dl_tab <- dl_tab[[1]][373:622]

  #dl_tab
  df <- dplyr::as_data_frame(dl_tab)
  
  df
  
  #Scraping all of the data and putting them into separate schools
  
  #Scraping Data for Stanford
  temp <- c(df[1,],df[3,])
  names(temp) <- c("University", "Location")
  df_stan <- as_data_frame(temp)
  df_stan <- separate(df_stan,University,c("rank","College"), sep = "[.]")
  
  df_final <- df_stan

  #Scraping Data for MIT
  temp <- c(df[11,],df[13,]) 
  names(temp) <- c("University", "Location")
  df_mit <- as_data_frame(temp)
  df_mit <- separate(df_mit,University,c("rank","College"), sep = "[.]")
  df_final <- bind_rows(df_final)
  
  #Scraping Data for Berkley
  temp <- c(df[25,],df[27,]) 
  names(temp) <- c("University", "Location")
  df_berk  <- as_data_frame(temp)
  df_berk  <- separate(df_berk ,University,c("rank","College"), sep = "[.]")

  #Scraping Data for Purdue
  temp <- c(df[37,],df[39,]) 
  names(temp) <- c("University", "Location")
  df_pr  <- as_data_frame(temp)
  df_pr  <- separate(df_pr ,University,c("rank","College"), sep = "[.]")
 
  #Scraping Data for CMU
  temp <- c("5. Carnegie Mellon (Pittsburgh)",df[54,]) 
  names(temp) <- c("University", "Location")
  df_cmu  <- as_data_frame(temp)
  df_cmu  <- separate(df_cmu ,University,c("rank","College"), sep = "[.]")
  
  
  #Scraping Data for Cornell
  temp <- c(df[63,],df[65,]) 
  names(temp) <- c("University", "Location")
  df_corn  <- as_data_frame(temp)
  df_corn  <- separate(df_corn ,University,c("rank","College"), sep = "[.]")

  
   #Scraping Data for USC
  temp <- c(df[73,],df[76,]) 
  names(temp) <- c("University", "Location")
  df_usc  <- as_data_frame(temp)
  df_usc  <- separate(df_usc ,University,c("rank","College"), sep = "[.]")
  
  #Scraping Data for UTA
  temp <- c(df[86,],df[88,]) 
  names(temp) <- c("University", "Location")
  df_uta  <- as_data_frame(temp)
  df_uta  <- separate(df_uta ,University,c("rank","College"), sep = "[.]")
  
  #Scraping Data for Harvard
  temp <- c(df[98,],df[100,]) 
  names(temp) <- c("University", "Location")
  df_harv  <- as_data_frame(temp)
  df_harv <- separate(df_harv ,University,c("rank","College"), sep = "[.]")

  #Scraping Data for CalTech
  temp <- c(df[110,],df[112,]) 
  names(temp) <- c("University", "Location")
  df_calt  <- as_data_frame(temp)
  df_calt <- separate(df_calt ,University,c("rank","College"), sep = "[.]")

 #Scraping Data for University of Chicago
  temp <- c(df[123,],df[125,]) 
  names(temp) <- c("University", "Location")
  df_uchic  <- as_data_frame(temp)
  df_uchic <- separate(df_uchic,University,c("rank","College"), sep = "[.]")

  #Scraping Data for UMD
  temp <- c(df[137,],df[139,]) 
  names(temp) <- c("University", "Location")
  df_umd   <- as_data_frame(temp)
  df_umd  <- separate(df_umd ,University,c("rank","College"), sep = "[.]")
  
  #Scraping Data for Unviersity of Carolina SD
  temp <- c(df[149,],df[151,]) 
  names(temp) <- c("University", "Location")
  df_ucsd   <- as_data_frame(temp)
  df_ucsd  <- separate(df_ucsd ,University,c("rank","College"), sep = "[.]")
  
  df_uma <- df[160:172,]
  #Scraping Data for University of Michigan Ann Arbor
  temp <- c(df[161,],df[163,]) 
  names(temp) <- c("University", "Location")
  df_uma   <- as_data_frame(temp)
  df_uma  <- separate(df_uma ,University,c("rank","College"), sep = "[.]")
  
  #Scraping Data for Purdue
  temp <- c(df[173,],df[176,]) 
  names(temp) <- c("University", "Location")
  df_purd   <- as_data_frame(temp)
  df_purd  <- separate(df_purd ,University,c("rank","College"), sep = "[.]")

  #Scraping Data for U Columbia
  temp <- c(df[186,],df[188,]) 
  names(temp) <- c("University", "Location")
  df_columb   <- as_data_frame(temp)
  df_columb  <- separate(df_columb ,University,c("rank","College"), sep = "[.]")

  #Scraping Data for FOU
  temp <- c(df[200,],df[202,]) 
  names(temp) <- c("University", "Location")
  df_fou   <- as_data_frame(temp)
  df_fou  <- separate(df_fou ,University,c("rank","College"), sep = "[.]")

  #Scraping Data for Georgia Tech
  temp <- c(df[214,],df[216,]) 
  names(temp) <- c("University", "Location")
  df_gtech   <- as_data_frame(temp)
  df_gtech  <- separate(df_gtech ,University,c("rank","College"), sep = "[.]")
  
  #Scraping Data for UCLA
  temp <- c("19. University of California Los Angeles",df[228,]) 
  names(temp) <- c("University", "Location")
  df_ucla  <- as_data_frame(temp)
  df_ucla <- separate(df_ucla ,University,c("rank","College"), sep = "[.]")
  
  #Scraping Data for Yale
  temp <- c(df[238,],df[240,]) 
  names(temp) <- c("University", "Location")
  df_yale   <- as_data_frame(temp)
  df_yale  <- separate(df_yale ,University,c("rank","College"), sep = "[.]")
  
 df_final <- bind_rows(df_stan,df_mit,df_pr,df_berk,df_cmu,df_corn,df_usc,df_uta,df_harv,df_calt,df_uchic,df_umd,df_ucsd,df_uma,df_purd,df_columb,df_fou,
                        df_gtech,df_ucla,df_yale)

 df_final <- df_final %>%
   mutate(lat = geocode_OSM(College, as.data.frame = TRUE)[[2]],lng = geocode_OSM(College, as.data.frame = TRUE)[[3]] )
 
 df_final



  #as_data_frame(dl_tab)


```


## Map of the Previous Data Frame
This map is extremely useful in being able to visualize the data that we have created and to show the locations of the top 20 graduate programs in the USA. As you scroll in , the colleges are identified by the emojis of universities. We used the leaflet library in order to create this map.

More information on how to create maps in R, take a look at https://rstudio.github.io/leaflet/markers.html. This website will give an in dept explanation on how to create a map and integrate markers that pin point certain locations. Additioanlly, we were able to find the geolocation(longitude and latitude) of a given college through the use of geocode_OSM. To learn more about this function, take a look at this - https://www.rdocumentation.org/packages/tmap/versions/1.6-1/topics/geocode_OSM .
```{r prep_map, message=FALSE}

schoolIcons <- icons(
  iconUrl = "https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/240/google/146/school_1f3eb.png",
  iconWidth = 22, iconHeight = 22
) 
grad_map <- leaflet(df_final) %>%
  addTiles() %>%
  setView(lat=37.9, lng=-96, zoom=4) %>%
  addMarkers(~lng,~lat,label=~College,icon = schoolIcons, clusterOptions = markerClusterOptions(), popup=paste("College:", df_final$College, "<br>",
                                                                                            "Location:",df_final$Location))
grad_map


```


# Data Curation

For the rest of our project, the dataset we will be using comes from a comma-separated values (CSV) file, found on https://www.kaggle.com/mohansacharya/graduate-admissions/downloads/graduate-admissions.zip/2

We can view the data by calling the read_csv command on the downloaded file.
```{r prep, message=FALSE}

csv <- "./Admission_Predict.csv"
df <- read_csv(csv)
df

```

# Management
Now that we have the data, we should understand what the components are.

## Missing Values
First things first, we need to check and make sure there are no missing values in the dataset.
We can use the apply function, which takes a variable, margin, and function. The variable in this case would be the function is.na applied on the dataset, df. This creates a matrix filled with boolean values for each value in the dataset, whether it is marked "NA" or not. We use a margin of 2 so the function is applied by column, and the function sum adds up the number of NA values in each column.

```{r missing}
apply(is.na(df),2, sum)

```

We find that our dataset has no missing values and can proceed.

## Entries
The entries for this dataframe are individuals who have applied to graduate school. Each entry, or row, corresponds to one person. 

## Attributes
There are several types of attributes in R, as each column is made up of values of a particular class. Some of the common data types can be found at https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/

The attributes (or columns) in the dataset are as follows: Serial No., GRE Score, TOEFL Score, University Rating, SOP, LOR, GPA, Research, and Chance of Admit

### Serial Number
The serial number of each entry merely identifies the row the entry is in. As a result, it does not need to be kept in the dataframe.
```{r remsno}
# remove first column containing Serial Number
tidy_data <- df[,-1]
tidy_data
```

### GRE Score
The GRE is the Graduate Record Examination, and serves as an admission test for graduate school. The exam is a broad assessment of one's critical thinking, analytica writing, verbal reasoning, and quantitative reasoning skills. The exam is scored from 130-170 in each of the two sections, Verbal and Quantitative, making the maximum possible score 340.

### TOEFL Score
The TOEFL is the test of English as a foreign language, taken by many international students. The test is scored on a scale of 0 to 120 and is made up of four sections: Reading, Listening, Speaking, and Writing. 

### University Rating
This is a rating out of 5 of the applicant's undergraduate university.

### Statement of Purpose (SOP)
The SOP is an essay written in the application, scored in this case from 1-5 in terms of strength.

### Letters of Recommendation (LOR)
The LOR are written by professors and mentors, and are scored in the dataset from 1-5 in terms of strength

### Cumulative GPA
The cumulative undergraduate GPA of the applicant is out of 10.0

### Research
The research attribute identifies whether or not an applicant conducted research prior to applying, with 1 for yes and 0 for no.
Since this is binary, we can turn the values into booleans, with true = 1 and false = 0

```{r research}
#convert 1's and 0's in Research to boolean values

tidy_data$Research <- as.logical(as.integer(tidy_data$Research))
tidy_data

```

### Chance of Admit
The chance of admittance into an American Graduate Program is scored from 0 to 1.


# Exploratory Data Analysis

Exploratory Data Analysis (EDA) serves as an initial investigation of attributes across entities. Some of the varaible properties we would like to understand are:
* central trends
* spread
* skew
* outliers
We can also look at relationships between variables, which will serve useful later when creating models.

## Visualization

We can get a general idea of an applicant's chance of acceptance in the dataset. By arranging all of the rows in order of Chance of Admit before plotting, we are able to see a general trend, which we can delve deeper into.
```{r simple_vis}
tidy_data %>%
  arrange(`Chance of Admit`) %>%
  rowid_to_column() %>%
  ggplot(aes(x=rowid, y=`Chance of Admit`)) +
  geom_point()
```

Based on the plot, we see a somewhat linear increase in chance, with a curve at the beginning. 

To know if the distribution is even, we can look at a box plot.
```{r boxplot}
tidy_data %>%
  ggplot(aes(x='', y=`Chance of Admit`)) +
  geom_boxplot()

```

This boxplot gives us a visualization of the central tendency (the line in the box), the spread (the box and whiskets) and outliers (the dots outside of the whiskers). 
We see the median chance of acceptance is around .7 for applicants in our dataset and the interquartile range appears to be pretty even on both sides. We can confirm these obervations with statistics.

We can also visualize pairs of variables. 
The GRE is used by thousands of graduate schools around the world, and serves as a standardized test for evaluating all applicants. However, we know that the test score is not the only thing that matters in terms of being accepted, and occasionally, poorer scoring students will still get into a school. 
To look at continuous variables GRE Score and Chance of Admit, we can produce a scatter plot. 
```{r scatter}

tidy_data %>%
  ggplot(aes(x=`GRE Score`, y=`Chance of Admit`)) +
  geom_point() + 
  ggtitle("Chance of Admit across GRE Scores")

```
From the scatter plot, we can see a pattern of Chance of Admit increasing with GRE Score. With a model, we will be better able to quantify the potential relationship.

## Summary Statistics

The summary statistics groups the data by GRE scores and then calculates the mean of admit. So the GRE_Scores_Interval groups the GRE Scores in groups of 5 from 290-340. Then, it calculates the mean of the chance of admit for each interval. As you can tell, the GRE Scores increase, the mean chance of admit increases as well. As you can tell the Mean chance of Admit range increases from .52 to .94 as the intervals increase.
```{r summary}

summary <- tidy_data %>%
           mutate("GRE_Scores_Interval" = cut(`GRE Score` ,breaks = 10)) %>% 
           group_by(GRE_Scores_Interval) %>%
           summarize("Mean Chance of Admit"=mean(`Chance of Admit`))

summary
```

## Data Transformations
Our data transformation consists of centering the data for the previous summary statistics that we performed on GRE vs Mean Chance of Admit. We will then break up the data by the standard deviation instead of by intervals of 5. As you can tell the standard deviations are from -3 to 3 since this was the range of the SD of the data. This then allows us to see the change in the average of the data as the standard deivation away from the mean changes.
```{r data_trans}


sd <- tidy_data %>% 
  summarize("Standard_Deviation"= sd(`GRE Score`),"Mean"=mean(`GRE Score`)) 
sd

summary <- tidy_data %>%
           mutate(z = (`GRE Score` - sd$Mean) / sd$Standard_Deviation)
summary 

summary %>%
           mutate("GRE Centered Score" = cut(z , seq(from= -3, to= 3, by= 1.0))) %>%
           group_by(`GRE Centered Score`) %>%
           summarise("Mean" = mean(`Chance of Admit`))



```

# Hypothesis Testing
Since I have mad multiple statements regarding the relationship of GRE Score and the probability of being accepted into a University, I decided to test a different relationship. I did Chance of Admit vs University Rating in order to see if there was a relationship between the rating of the university you attended and the Chance that was calculated of admission. As you can see, the p-value for this relationship is significantly low at 6.53 * 10 ^-63 which is less than .05, allowing us to reject the null hypothesis.

```{r hypo}

library(tidyverse)
hypo <- tidy_data %>% lm(formula=`University Rating`~`Chance of Admit`)
broom::tidy(hypo)

```

# Machine Learning
As students, we want to see what can maximize our chances of getting into a Graduate Program. Given that there are multiple factors in the dataset that may improve chances, we can conduct a linear regression to see if there is a relationship between any of the attributes.

## Simple Regression
To look at the relationship between two continuous variables, we can assume that in our population, the relationship between them is given by the linear function
$Y = \beta{_0} + \beta{_1}X \\$

Since we are interested in how chance of being admitted is affected by factors (ex: GRE score), we can do the following regression: 
$Chance of Admit \approx \beta{_0} + \beta{_1}\times GRE Score \\$


In the above graphs used in EDA Visualization, we used components for adding geom_points and geom_boxplots. However, multiple components can be added to a single plot. For example, a geom_smooth component can be layered, Additional information can be found at https://ggplot2.tidyverse.org/reference/gg-add.html

```{r simple}
simple_reg <- tidy_data %>%
  ggplot(aes(x=`GRE Score`, y= `Chance of Admit`)) +
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle("Chance of Admit across GRE Scores")
simple_reg

# The linear model equation
gre_fit <- lm(`Chance of Admit`~`GRE Score`, data=tidy_data)
gre_fit
```

According to the lm function, $\hat\beta{_0} = -2.436084\\$ and $\hat\beta{_1} = 0.009976\\$ 
This means that on average, an applicant's chance of admittance would increase by 0.01 for every additional point scored on the GRE.

Now that we have a linear model, we must interpret how well it fits the data.
First, we need to check that there is a relationship between Chance of Admit and GRE Score. This can be done with a hypothesis test. 
$\ H{_0}; \beta{_1} = 0\\$ meaning there is no relationship
$\ H{_0}; \beta{_1} \neq 0\\$

```{r htest}
gre_fit %>%
  tidy()
```

We find that our p value for $\hat\beta{_1}\\$ is much lower than .05, our cutoff for significance. Thus we can state that there is a statistically significant relationship between GRE Score and Chance of Admittance.

## Interaction
We know that GRE is a factor for admittance, but what if there are more? In particular, what if a combination of multiple factors gives us a better model for the data?
Interaction models allow us to see if the effect of one predictor on the outcome is increased by another predictor.

Graduate schools provide students with more advanced learning in a specialized field. Much of this comes from doing hands on research in their respective programs. 
While it is not mandatory to conduct research during undergraduate studies, there is a notion that doing so will greatly help your chances of getting into a good school. 

Therefore, another variable we can consider is whether an applicant has done research or not. 
This would require models of conditional expectation, stiill represented as a linear function.
$Chance of Admit \approx \beta{_0} + \beta{_1}\times GRE Score + \beta{_2}\times Research + \beta{_3}\times (GRE Score \times Research)\\$

Like in simple regression, we can build our model using geom_smooth

```{r interaction}
tidy_data %>%
  ggplot(aes(x=`GRE Score`, y=`Chance of Admit`, color=Research)) +
    geom_point() +
    geom_smooth(method=lm) + 
    ggtitle("Chance of Admit across GRE Score by Research Conducted")

```

The two lines are similar, but the slope of Research==TRUE appears to be greater.

Now that we have the model, we can see how it fits.
```{r multiple}
multiple_fit <- lm(`Chance of Admit`~`GRE Score`*Research, data=tidy_data)
multiple_fit_stats <- multiple_fit %>%
  tidy()
multiple_fit_stats %>% knitr::kable()
```

We can see that for Research==TRUE and FALSE, the relationships between GRE Score and Chance of Admit are not significantly different. This is because while the estimates for GREScore and GREScore:ResearchTRUE are different, the pvalue of 0.0086 is great enough that they may be equal. 
Normally, we do not require a pvalue to be below 0.05, but the estimates for the effect of Research on GRE Score are only 0.005 off. As a result, we need our pvalue to be lower than the difference so that we can make sure the confidence interval does not overlap. However, our pvalue for GRE Score:ResearchTRUE is 0.0059, greater than the difference.

## ANOVA

An Analysis of Variance Test allows us to compare models and see which one is a better fit for the data.

```{r anova, warning=FALSE}
anova(gre_fit, multiple_fit)

```

The pvalue calculated is 3.363e-06, which is incredibly small. Thus we can state that the second model with the GRE Score and Research interaction is a better fit for the data than the model with just GRE Score.

